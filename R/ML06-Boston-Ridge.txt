###
###
### ML Regularization - Boston Data
###       LASSO and Ridge Regression
###
#################################


library(MASS)      # install.packages("MASS")

data(Boston)

library(tidyverse)   # install.packages(tidyverse")
Boston <- as_tibble(Boston)
Boston

?Boston  # see explanation for variables

#1  crim     per capita crime rate by town.
#2  zn       proportion of residential land zoned for lots over 25,000 sq.ft.
#3  indus    proportion of non-retail business acres per town.
#4  chas     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
#5  nox      nitrogen oxides concentration (parts per 10 million).
#6  rm       average number of rooms per dwelling.
#7  age      proportion of owner-occupied units built prior to 1940.
#8  dis      weighted mean of distances to five Boston employment centres.
#9  rad      index of accessibility to radial highways.
#10 tax      full-value property-tax rate per $10,000.
#11 ptratio  pupil-teacher ratio by town.
#12 black    1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
#13 lstat    lower status of the population (percent).
#14 medv     median value of owner-occupied homes in \$1000s.


# 13 lstat = lower status of the population (percent)
#      Proportion of population that is lower status
#      = 1/2 (proportion of adults without,
#      some high school education and proportion of male
#      workers classified as laborers). The logarithmic
#      specification implies that socioeconomic status
#      distinctions mean more in the upper brackets of
#      society than in the lower classes. Source: 1970 U. S. Census



#- Correlaiton Plots
pairs(Boston)   # shows scatterplot matrix (large)


library(corrplot)   # install.packages("corrplot")
corrplot(cor(Boston), method="number")
corrplot(cor(Boston))



#- Histogram of the respoonse variable
attach(Boston)
hist(medv)

#1  crim
#2  zn
#3  indus
#4  chas
#5  nox
#6  rm
#7  age
#8  dis
#9  rad
#10 tax
#11 ptratio
#12 black
#13 lstat
#14 medv  <- response (Y)



#- Regression with all variables
Reg01 <- lm(medv ~ . , data=Boston)
summary(Reg01)



# Change class of default and student column
Boston2 <- Boston %>% mutate( chas=as.numeric(chas),
                                rad=as.numeric(rad))
Boston2

# Standardize the data for Lasso and Ridge Reg
Boston3 <- Boston2 %>% scale(center=TRUE, scale=TRUE)
Boston3 <- as_tibble(Boston3)
Boston3


###--- 2. Data Separation (Copied from Rtut-CV)

### --- Divide Dataset to Training and Testing and Set up k fold CV
Orig <- Boston3               # Entire Data set (have to be data.frame)
train.size <- 400            # num of rows for training set
test.size <- 96              # num of rows for testing set
resp.col.name <- "medv"      # name of response column
num.folds <- 5               # k for k-fold CV
my.seed <- 16442             # give a seed


    #---
    set.seed(my.seed)
    ix = sample(1:nrow(Orig))
    Orig2 = Orig[ix, ]
    Train.set  = Orig2[1:train.size, ]
    Train.resp = Orig2[1:train.size, resp.col.name]
    Test.set   = Orig2[(train.size+1):(train.size+test.size), ]
    Test.resp  = Orig2[(train.size+1):(train.size+test.size), resp.col.name]

    # K-fold Cross Validation
    library(cvTools)     # install.packages("cvTools")
    set.seed(my.seed)
    folds = cvFolds(  nrow(Train.set),  K=num.folds  )  # k-fold CV (random assignment)

    CV.train      = list(Train.set[ folds$which!=1, ])
    CV.train.resp = list(Train.resp[folds$which!=1,1])
    CV.valid      = list(Train.set[ folds$which==1, ])
    CV.valid.resp = list(Train.resp[folds$which==1,1])

    for (k in 2:num.folds) {
      CV.train[[k]]      = Train.set[ folds$which!=k, ]
      CV.train.resp[[k]] = Train.resp[folds$which!=k,1]
      CV.valid[[k]]      = Train.set[ folds$which==k, ]
      CV.valid.resp[[k]] = Train.resp[folds$which==k,1]
    }
# Output (all data.frame):
#   Train.set      /  Train.resp
#   Test.set       /  Test.resp
#   CV.train[[k]]  /  CV.train.resp[[k]]
#   CV.valid[[k]]  /  CV.valid.resp[[k]]





###-------------------------------------------------
###--- 2a. Ordinary Regression (Entire Training set)
Fit01 = lm(medv ~. , data=Train.set)
summary(Fit01)

# predict values medv in Test set
pred = predict(Fit01, newdata = Test.set)

library(caret)          # install.packages("caret")
OLS <- data.frame(
  RMSE = RMSE(pred, Test.set$medv),
  Rsquare = R2(pred, Test.resp$medv)
)
OLS




###-------------------------------------------------
###--- 2b. Lasso Regression (Entire Training set. Auto 5-fold CV)
library(glmnet)          # install.packages("glmnet")

# Display the best lambda value
x <- model.matrix(medv~., Train.set)[,-1]
y <- Train.set$medv
set.seed(my.seed)
CV.for.lambda <- cv.glmnet(x, y, alpha = 1, nfolds=5)
CV.for.lambda$lambda.min
FitLasso <- glmnet(x, y, alpha = 1, lambda = CV.for.lambda$lambda.min)
coef(FitLasso)

x.test <- model.matrix(medv ~., Test.set)[,-1]
pred <- as.vector(predict(FitLasso, x.test))

# Model performance metrics
LASSO <- data.frame(
  RMSE = RMSE(pred, as.matrix(Test.resp)),
  Rsquare = R2(pred, Test.resp)
)
LASSO





###-------------------------------------------------
###--- 2c. Ridge Regression (Entire Training set. Auto 5-fold CV)

# Display the best lambda value
x <- model.matrix(medv~., Train.set)[,-1]
y <- Train.set$medv
set.seed(my.seed)
CV.for.lambda <- cv.glmnet(x, y, alpha = 0, nfolds=5)
CV.for.lambda$lambda.min
FitRidge <- glmnet(x, y, alpha = 0, lambda = CV.for.lambda$lambda.min)
coef(FitRidge)

x.test <- model.matrix(medv ~., Test.set)[,-1]
pred <- as.vector(predict(FitRidge, x.test))

# Model performance metrics
RIDGE <- data.frame(
  RMSE = RMSE(pred, as.matrix(Test.resp)),
  Rsquare = R2(pred, Test.resp)
)
RIDGE


OLS
LASSO
RIDGE
