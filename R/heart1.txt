###
###  Heart Data - Logistic Regression
###
###    ver 0.0.3
####################################################

###-------------------------------------------------------
###--- 0. Preliminary

# Using Heart2 from https://nmimoto.github.io/R/heart0.txt
library(tidyverse)
Heart <- read_csv(file="https://nmimoto.github.io/datasets/heart.csv")

# Rename medv column as resp.
Heart2 <- Heart %>%
    select(-"index")  %>%            # remove col named "index"
    rename(resp=AHD) %>%             # rename the column
    relocate(resp) %>%               # move "resp" to 1st column
    mutate(resp=as.factor(resp),     # Turn these columns to <factor> instead of <double>
           ChestPain=as.factor(ChestPain),
           Thal=as.factor(Thal),
           Sex=as.factor(Sex),
           Fbs=as.factor(Fbs),
           RestECG=as.factor(RestECG),
           ExAng=as.factor(ExAng))

print(Heart2, width=1000)
table(Heart2$resp)    # note that first level is "No"


# Low p-value from Chi-sq test of association
#  3 4 8 10 11 12 13 14
#  "Sex" "ChestPain" "RestECG" "ExAng" "Oldpeak" "Slope" "Ca" "Thal"
# High p-value from Chi-sq test of association
#  7
#  "Fbs"

# need to remove rows with NA.
Orig <- Heart2

#- Check for N/A in data. Remove if there's any.
  summary(Orig)
  dim(Orig)
  sum(is.na(Orig))
  # If there is na in the data, run below
  Orig <- Orig %>% na.omit()
  dim(Orig)


###--------------------------------------------------------------------
###--- 2. Divide Dataset to Training and Testing and Set up k fold CV
Orig <- Orig                 # Entire Data set (have to be data.frame)
train.size <- 250            # num of rows for training set
test.size <-  47             # num of rows for testing set
resp.col.name <- "resp"      # name of response column
num.folds <- 5               # k for k-fold CV
my.seed <- 7211             # give a seed

    #---
    set.seed(my.seed)
    ix = sample(1:nrow(Orig))
    Orig2 = Orig[ix, ]
    Train.set  = Orig2[1:train.size, ]
    Train.resp = Orig2[1:train.size, resp.col.name]
    Test.set   = Orig2[(train.size+1):(train.size+test.size), ]
    Test.resp  = Orig2[(train.size+1):(train.size+test.size), resp.col.name]

    # K-fold Cross Validation
    library(cvTools)     # install.packages("cvTools")
    set.seed(my.seed)
    folds = cvFolds(  nrow(Train.set),  K=num.folds  )  # k-fold CV (random assignment)

    CV.train      = list(Train.set[ folds$which!=1, ])
    CV.train.resp = list(Train.resp[folds$which!=1,1])
    CV.valid      = list(Train.set[ folds$which==1, ])
    CV.valid.resp = list(Train.resp[folds$which==1,1])

    for (k in 2:num.folds) {
      CV.train[[k]]      = Train.set[ folds$which!=k, ]
      CV.train.resp[[k]] = Train.resp[folds$which!=k,1]
      CV.valid[[k]]      = Train.set[ folds$which==k, ]
      CV.valid.resp[[k]] = Train.resp[folds$which==k,1]
    }
# Output (all data.frame):
#   Train.set      /  Train.resp
#   Test.set       /  Test.resp
#   CV.train[[k]]  /  CV.train.resp[[k]]
#   CV.valid[[k]]  /  CV.valid.resp[[k]]


# Visualization doesn't help much here
ix_no = (Train.set$resp=="No")
ix_yes = (Train.set$resp=="Yes")
plot(Train.set$RestBP[ix_no], Train.set$Chol[ix_no], xlab="RestBp", ylab="Chol")
lines(Train.set$RestBP[ix_yes], Train.set$Chol[ix_yes], col="red", type="p")





###-------------------------------------------------
###--- 3. Logistic Regression

###--------------------
###--- 3.a Logistic Regression for 1 fold
k=1
Fit01 <- glm(resp ~. , family=binomial, data=CV.train[[k]] )

#- Extract fitted response (training)
Train.prob =predict(Fit01, type ="response")  # fitted responses
#- Predict in Validation Set
Valid.prob = predict(Fit01, newdata=CV.valid[[k]], type="response")

#- Pick a threshold
threshold = .5

    #- Check the training set accuracy
    library(caret)
    Train.pred = ifelse(Train.prob > threshold, "Yes", "No")  # Turn the fitted values to Up/Down using threshold of .5
    Valid.pred = ifelse(Valid.prob > threshold, "Yes", "No")
    CM.train <- confusionMatrix(factor(Train.pred),  factor(as.matrix(CV.train.resp[[k]])), positive="Yes")
    CM.valid  <- confusionMatrix(factor(Valid.pred), factor(as.matrix(CV.valid.resp[[k]])), positive="Yes")

    CM.train[["byClass"]][["Sensitivity"]] # Pick out sensitivity
    CM.train[["byClass"]][["Specificity"]] # Pick out specificity
    CM.train            # Training set result
    CM.train$table      # output just the table
    CM.valid[["byClass"]][["Sensitivity"]] # Pick out sensitivity
    CM.valid[["byClass"]][["Specificity"]] # Pick out specificity
    CM.valid             # Testing set
    CM.valid$table       # output just the table

    colSums(CM.valid$table) / sum(colSums(CM.valid$table))    # % of Actual Yes/No
    rowSums(CM.valid$table) / sum(rowSums(CM.valid$table))    # % of predicted Yes/No

    ###--- Check ROC curve and AUC
    library(pROC)
    layout(matrix(1:2,1,2))
      #- Training Set ROC
      plot.roc(factor(as.matrix(CV.train.resp[[k]])),  Train.prob, levels=c("No", "Yes"), main="Training")
        # point corresponding to CM.train
        abline(h=CM.train[["byClass"]][["Sensitivity"]], v=CM.train[["byClass"]][["Specificity"]], col="red")
        auc.train = auc(factor(as.matrix(CV.train.resp[[k]])), Train.prob, levels=c("No", "Yes"))
        text(.2, .2, paste("AUC=",round(auc.train, 3)))

      #- Test Set ROC
      plot.roc(factor(as.matrix(CV.valid.resp[[k]])),  Valid.prob, levels=c("No", "Yes"), main="Test")
        # point corresponding to CM.valid
        abline(h=CM.valid[["byClass"]][["Sensitivity"]], v=CM.valid[["byClass"]][["Specificity"]], col="red")
        auc.valid = auc(factor(as.matrix(CV.valid.resp[[k]])), Valid.prob, levels=c("No", "Yes"))
        text(.2, .2, paste("AUC=",round(auc.valid, 3)))
    layout(1)


###
### Now we need to change k from 1-5and run again.
###


###--- Plot the fitted curve for visualization (not useful)
plot(CV.train[[k]]$RestBP,  (CV.train[[k]]$resp=="Yes"), xlab="RestBP", ylab="AHD")
lines(CV.train[[k]]$RestBP, Fit01$fitted, lwd=2, col="red", type="p")





###--------------------
###--- 3.b Logistic Regression for all folds k=1:5
AUCs <- MSE.valid <- matrix(0, 5, 2)
colnames(AUCs) = c("Train AUC", "Valid AUC")
for (k in 1:5) {

    Fit01 <- glm(resp ~. , family=binomial, data=CV.train[[k]])     # <----- Change model here

    #- Extract fitted response (training)
    Train.prob =predict(Fit01, type ="response")  # fitted responses
    #- Predict in Validation Set
    Valid.prob = predict(Fit01, newdata=CV.valid[[k]], type="response")


    #- Check the training set accuracy
    library(caret)
    Train.pred = ifelse(Train.prob > threshold, "Yes", "No")  # Turn the fitted values to Up/Down using threshold of .5
    Valid.pred = ifelse(Valid.prob > threshold, "Yes", "No")
    #CM.train <- confusionMatrix(factor(Train.pred),  factor(as.matrix(CV.train.resp[[k]])), positive="Yes")
    #CM.valid  <- confusionMatrix(factor(Valid.pred), factor(as.matrix(CV.valid.resp[[k]])), positive="Yes")

    AUCs[k,] <- round(c(auc(factor(as.matrix(CV.train.resp[[k]])), Train.prob, levels=c("No", "Yes")),
                        auc(factor(as.matrix(CV.valid.resp[[k]])), Valid.prob, levels=c("No", "Yes"))), 4)

}
AUCs

Av.AUCs = apply(AUCs, 2, mean)
Av.AUCs


###
###   Make decision about best model based on Av Valid AUC
###




###-----------------------------------------------------------
###--- 4. Final Fit with Logistic Regression for Training Set (No CV. Just Training vs Test)

## [ Train.set 250     ]     [Test.set 47]
## [50][50][50][50][50]

Fit01 <- glm(resp ~ . -Chol -Age -Fbs -ExAng -RestECG, family=binomial, data=Train.set)

#- Extract fitted response (training)
Train.prob =predict(Fit01, type ="response")  # fitted responses
#- Predict in Validation Set
Test.prob = predict(Fit01, newdata=Test.set, type="response")


#- Pick a threshold
threshold = .5

    #- Check the training set accuracy
    library(caret)
    Train.pred = ifelse(Train.prob > threshold, "Yes", "No")  # Turn the fitted values to Up/Down using threshold of .5
    Test.pred  = ifelse(Test.prob > threshold, "Yes", "No")
    CM.train <- confusionMatrix(factor(Train.pred),  factor(as.matrix(Train.resp)), positive="Yes")
    CM.test  <- confusionMatrix(factor(Test.pred), factor(as.matrix(Test.resp)), positive="Yes")

    CM.train[["byClass"]][["Sensitivity"]] # Pick out sensitivity
    CM.train[["byClass"]][["Specificity"]] # Pick out specificity
    CM.train            # Training set result
    CM.train$table      # output just the table
    CM.test[["byClass"]][["Sensitivity"]] # Pick out sensitivity
    CM.test[["byClass"]][["Specificity"]] # Pick out specificity
    CM.test              # Testing set
    CM.test$table       # output just the table

    colSums(CM.test$table) / sum(colSums(CM.test$table))    # % of Actual Yes/No
    rowSums(CM.test$table) / sum(rowSums(CM.test$table))    # % of predicted Yes/No

    ###--- Check ROC curve and AUC
    library(pROC)
    layout(matrix(1:2,1,2))
      #- Training Set ROC
      plot.roc(factor(as.matrix(Train.resp)),  Train.prob, levels=c("No", "Yes"), main="Training")
        # point corresponding to CM.train
        abline(h=CM.train[["byClass"]][["Sensitivity"]], v=CM.train[["byClass"]][["Specificity"]], col="red")
        auc.train = auc(factor(as.matrix(Train.resp)), Train.prob, levels=c("No", "Yes"))
        text(.2, .2, paste("AUC=",round(auc.train, 3)))

      #- Test Set ROC
      plot.roc(factor(as.matrix(Test.resp)),  Test.prob, levels=c("No", "Yes"), main="Test")
        # point corresponding to CM.valid
        abline(h=CM.valid[["byClass"]][["Sensitivity"]], v=CM.valid[["byClass"]][["Specificity"]], col="red")
        auc.test = auc(factor(as.matrix(Test.resp)), Test.prob, levels=c("No", "Yes"))
        text(.2, .2, paste("AUC=",round(auc.valid, 3)))
    layout(1)


###
### AUC.test is the estimate for this model's prediction power. (with threshold = .5)
### ROC curve represents the overall capability of this model.
### By changing threshold, we can be at anywhere on the curve.
###




###-----------------------------------------------------------
###--- Threshold Picker

#    Cost for (TP, TN, FP, FN)
# cost.list = c(0,  0,  3,  1) / 4
  cost.list =  c(0,  0,  2,  1) / 3
# cost.list = c(0,  0,  1,  1) / 2
# cost.list = c(0,  0,  1,  2) / 3
# cost.list = c(0,  0,  1,  3) / 4


threshold.list = seq(0.01,.99,.01)    # grid for threshold
cost=0
library(caret)      # for confusionMatrix
for (i in 1:length(threshold.list)){

    threshold = threshold.list[i]

    #- Check the training set accuracy
    Test.pred  = ifelse(Test.prob  > threshold, "Yes", "No")
    CM.test  <- confusionMatrix(factor(Test.pred),
                                factor(as.matrix(Test.resp)),
                                positive="Yes")
    TP = CM.test$table[2,2]   # True  Pos
    TN = CM.test$table[1,1]   # True  Neg
    FP = CM.test$table[2,1]   # False Pos
    FN = CM.test$table[1,2]   # False Neg

    cost[i] = sum(c(TP, TN, FP, FN) * cost.list)
}
plot(threshold.list, cost, xlab="threshold")

cost.list
which.min(cost)
min(cost)
threshold.list[which.min(cost)]
